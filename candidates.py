# candidates.py - GELÄ°ÅTÄ°RÄ°LMÄ°Å paralel scraping ve filtreleme
import time
import hashlib
from typing import List, Dict, Tuple, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from data import products as local_products
from web_search import search_products_on_web
from normalize import normalize_web_result, parse_query
from utils import normalize_category
from logger import get_logger
from scraper import scrape_product_page
import re

logger = get_logger("candidates")

# Perakendeci domainleri
RETAILER_DOMAINS = [
    "hepsiburada.com", "trendyol.com", "n11.com", "vatanbilgisayar.com",
    "mediamarkt.com.tr", "amazon.com.tr","incehesap.com",
    "itopya.com", "gamegaraj.com", "gaming.gen.tr",
]

CATEGORY_SITES: Dict[str, List[Tuple[str, str]]] = {
    "Telefon": [
        ("trendyol.com", "cep-telefonu"),
        ("hepsiburada.com", "cep-telefonlari"),
    ],
    "Laptop": [
        ("hepsiburada.com", "dizustu-bilgisayar"),
        ("trendyol.com", "laptop"),
    ],
    "MasaÃ¼stÃ¼": [
        ("itopya.com", "hazir-sistem"),
        ("incehesap.com", "gaming-pc"),
    ],
}

CONTENT_BLOCKLIST = ["epey.com", "versus.com", "donanimhaber.com"]

# Daha az kÄ±sÄ±tlayÄ±cÄ± blacklist
IRRELEVANT_PRODUCT_KEYWORDS = [
    "soÄŸutucu", "cooling", "cooler", "stand", "mousepad", "mouse pad",
    "kÄ±lÄ±f", "Ã§anta", "kablo", "ÅŸarj", "adaptÃ¶r", 
    "temizlik", "clean", "koruyucu", "protector", "film",
    "sticker", "Ã§Ä±kartma", "skin"
]

# YENÄ°: YenilenmiÅŸ/ikinci el Ã¼rÃ¼nleri engellemek iÃ§in anahtar kelimeler
REFURBISHED_KEYWORDS = [
    "yenilenmiÅŸ", "refurbished", "ikinci el", "2. el", "teÅŸhir", "outlet"
]

# Ana Ã¼rÃ¼n kategori kontrolÃ¼
MAIN_PRODUCT_KEYWORDS = {
    "laptop": ["laptop", "notebook", "bilgisayar", "pc", "gaming", "i5", "i7", "rtx", "gtx"],
    "telefon": ["telefon", "phone", "smartphone", "iphone", "android"],
    "tablet": ["tablet", "ipad"],
    "masaÃ¼stÃ¼": ["masaÃ¼stÃ¼", "desktop", "pc", "bilgisayar", "gaming"]
}

# Bellek iÃ§i Ã¶nbellek
_CACHE = {}
CACHE_TIMEOUT_SECONDS = 3600

# Paralel scraping konfigÃ¼rasyonu
MAX_WORKERS = 4 
SCRAPING_TIMEOUT = 30 
# DEÄÄ°ÅÄ°KLÄ°K: Daha fazla URL kazÄ±mak iÃ§in limiti artÄ±rdÄ±k
MAX_SCRAPE_ATTEMPTS = 15

def _dedupe_key(p: Dict[str, Any]) -> str:
    """ÃœrÃ¼nleri isme ve markaya gÃ¶re tekileÅŸtirmek iÃ§in bir anahtar oluÅŸturur."""
    name = (p.get("name") or "").strip().lower()
    brand = (p.get("brand") or "").strip().lower()
    return hashlib.sha1(f"{name}|{brand}".encode("utf-8")).hexdigest()

def _ensure_local_source(p: Dict[str, Any]) -> Dict[str, Any]:
    """Yerel Ã¼rÃ¼n verisine 'source' ekler."""
    p_copy = p.copy()
    p_copy["source"] = "local_database"
    return p_copy

def _clean_hepsiburada_url(url: str) -> str:
    """
    DÃœZELTÄ°LDÄ°: Daha gÃ¼venilir URL temizleme
    """
    if 'hepsiburada.com' not in url:
        return url
    
    if 'HBC' in url:
        if '-p-' not in url:
            parts = url.split('-')
            if len(parts) > 1 and (parts[-1].startswith('HBC') or 'HBCV' in parts[-1]):
                product_id_part = parts[-1].split('?')[0] # Query parametrelerini temizle
                clean_url = '-'.join(parts[:-1]) + f'-p-{product_id_part}'
                logger.debug(f"HB URL temizlendi: {url} -> {clean_url}")
                return clean_url
    
    return url

def _log_filtering_decision(product_name: str, reason: str, passed: bool):
    """Debug iÃ§in filtreleme kararlarÄ±nÄ± logla"""
    status = "âœ… GEÃ‡TÄ°" if passed else "âŒ ELENDÄ°"
    logger.debug(f"{status}: {product_name[:50]}... | Sebep: {reason}")

def _is_relevant_product(product_name: str, target_category: str) -> bool:
    """
    GÃœNCELLENDÄ°: Daha akÄ±llÄ± ve Ã§ok kategorili kategori kontrolÃ¼
    """
    if not product_name:
        return False
    
    name_lower = product_name.lower()
    
    # 1. ADIM (YENÄ°): YenilenmiÅŸ/Ä°kinci el Ã¼rÃ¼nleri en baÅŸta engelle
    for refurbished_keyword in REFURBISHED_KEYWORDS:
        if refurbished_keyword in name_lower:
            _log_filtering_decision(product_name, f"YenilenmiÅŸ Ã¼rÃ¼n: {refurbished_keyword}", False)
            return False
    
    # 2. ADIM: AÃ§Ä±k alakasÄ±z Ã¼rÃ¼nleri engelle
    for irrelevant_keyword in IRRELEVANT_PRODUCT_KEYWORDS:
        if irrelevant_keyword in name_lower:
            _log_filtering_decision(product_name, f"AlakasÄ±z keyword: {irrelevant_keyword}", False)
            return False
    
    # 3. ADIM: Kategori spesifik akÄ±llÄ± kontrol
    if target_category and target_category.lower() == "laptop":
        laptop_positive = ["laptop", "notebook", "gaming", "taÅŸÄ±nabilir", "portable"]
        has_laptop_signal = any(signal in name_lower for signal in laptop_positive)
        
        desktop_negative = ["masaÃ¼stÃ¼", "desktop", "hazÄ±r sistem", "gaming pc", "masa Ã¼stÃ¼"]
        has_desktop_signal = any(signal in name_lower for signal in desktop_negative)
        
        if has_desktop_signal and not has_laptop_signal:
            _log_filtering_decision(product_name, "MasaÃ¼stÃ¼ Ã¼rÃ¼n tespit edildi", False)
            return False
            
        if not has_laptop_signal:
            tech_keywords = ["inc", "inÃ§", "15.6", "14", "17.3", "hz", "taÅŸÄ±nabilir"]
            has_laptop_tech = any(keyword in name_lower for keyword in tech_keywords)
            
            if not has_laptop_tech:
                _log_filtering_decision(product_name, "Laptop sinyali bulunamadÄ±", False)
                return False
    
    # YENÄ°: MasaÃ¼stÃ¼ iÃ§in daha sÄ±kÄ± kontrol
    elif target_category and target_category.lower() == "masaÃ¼stÃ¼":
        desktop_positive = ["masaÃ¼stÃ¼", "desktop", "hazÄ±r sistem", "gaming pc", "oyuncu bilgisayarÄ±"]
        has_desktop_signal = any(signal in name_lower for signal in desktop_positive)
        
        laptop_negative = ["laptop", "notebook", "dizÃ¼stÃ¼", "taÅŸÄ±nabilir", "portable", " inÃ§", '"']
        has_laptop_signal = any(signal in name_lower for signal in laptop_negative)

        if has_laptop_signal and not has_desktop_signal:
            _log_filtering_decision(product_name, "Laptop Ã¼rÃ¼n tespit edildi (MasaÃ¼stÃ¼ bekleniyordu)", False)
            return False

    # YENÄ°: Telefon iÃ§in daha sÄ±kÄ± kontrol
    elif target_category and target_category.lower() == "telefon":
        phone_positive = ["telefon", "phone", "galaxy", "iphone", "xiaomi", "redmi"]
        has_phone_signal = any(signal in name_lower for signal in phone_positive)
        
        tablet_negative = ["tablet", "ipad", "tab"]
        has_tablet_signal = any(signal in name_lower for signal in tablet_negative)

        if has_tablet_signal and not has_phone_signal:
            _log_filtering_decision(product_name, "Tablet Ã¼rÃ¼n tespit edildi (Telefon bekleniyordu)", False)
            return False
    
    # 4. ADIM: DiÄŸer kategoriler iÃ§in genel kontrol
    elif target_category and target_category.lower() in MAIN_PRODUCT_KEYWORDS:
        category_keywords = MAIN_PRODUCT_KEYWORDS[target_category.lower()]
        has_main_product = any(keyword in name_lower for keyword in category_keywords)
        
        if not has_main_product:
            tech_keywords = ["gb", "tb", "ssd", "ram", "intel", "amd", "nvidia", "hz", "inc"]
            has_tech_specs = any(keyword in name_lower for keyword in tech_keywords)
            
            if not has_tech_specs:
                _log_filtering_decision(product_name, "Ne kategori ne de teknik Ã¶zellik", False)
                return False
    
    # 5. ADIM: Minimum uzunluk kontrolÃ¼
    if len(product_name.strip()) < 8:
        _log_filtering_decision(product_name, "Ã‡ok kÄ±sa Ã¼rÃ¼n adÄ±", False)
        return False
    
    _log_filtering_decision(product_name, "TÃ¼m kontrolleri geÃ§ti", True)
    return True

def _is_price_reasonable(price: Optional[int], budget: Optional[float], tolerance: float = None) -> bool:
    """
    DÃœZELTÄ°LDÄ°: Dinamik fiyat toleransÄ± - YENÄ° VE DAHA SIKI KURALLAR
    """
    if not budget or not price:
        return True
    
    if tolerance is None:
        # YENÄ° MANTIK: BÃ¼tÃ§e arttÄ±kÃ§a toleransÄ± dÃ¼ÅŸÃ¼rerek daha isabetli sonuÃ§lar al
        if budget <= 20000:
            tolerance = 0.20  # %20 (Ã¶rn: 15k iÃ§in 12k-18k aralÄ±ÄŸÄ±)
        else:
            tolerance = 0.15  # %15 (Ã¶rn: 40k iÃ§in 34k-46k aralÄ±ÄŸÄ±) -> Daha mantÄ±klÄ±!
    
    lower_bound = budget * (1 - tolerance)
    upper_bound = budget * (1 + tolerance)
    
    is_reasonable = lower_bound <= price <= upper_bound
    
    if not is_reasonable:
        logger.debug(f"Fiyat bÃ¼tÃ§e dÄ±ÅŸÄ±: {price} TL (beklenen: {lower_bound:.0f}-{upper_bound:.0f}, tolerans: %{tolerance*100:.0f})")
    
    return is_reasonable

def _scrape_single_url(url_data: Tuple[str, str, str]) -> Optional[Dict[str, Any]]:
    """
    Tek bir URL'yi scrape eden fonksiyon (paralel execution iÃ§in)
    """
    url, category, query = url_data
    
    try:
        # URL temizleme
        cleaned_url = _clean_hepsiburada_url(url)
        logger.debug(f"Scraping baÅŸlatÄ±ldÄ±: {cleaned_url}")
        
        # Scraping iÅŸlemi
        scraped_data = scrape_product_page(cleaned_url)
        
        # GÃœNCELLENDÄ°: Daha detaylÄ± loglama
        if not scraped_data:
            logger.debug(f"Scraping verisi boÅŸ dÃ¶ndÃ¼: {cleaned_url}")
            return None

        # URL'i gÃ¼ncelle
        scraped_data["url"] = cleaned_url
        scraped_data["original_query"] = query

        # Temel validasyon
        product_name = scraped_data.get("name") or ""
        price = scraped_data.get("price")
        
        # GÃœNCELLENDÄ°: Hangi verinin eksik olduÄŸunu belirt
        if not product_name:
            logger.debug(f"Eksik veri: ÃœrÃ¼n adÄ± bulunamadÄ±. URL: {cleaned_url}")
            return None
        if not price:
            logger.debug(f"Eksik veri: Fiyat bulunamadÄ±. URL: {cleaned_url}")
            return None
        
        logger.info(f"âœ… Scraping baÅŸarÄ±lÄ±: {product_name[:50]}... - {price} TL")
        return scraped_data
        
    except Exception as e:
        logger.warning(f"Scraping hatasÄ± {url}: {str(e)}")
        return None

def _fetch_and_filter_web_candidates_parallel(parsed_query: Any) -> List[Dict[str, Any]]:
    """
    DÃœZELTÄ°LDÄ°: Daha akÄ±llÄ± filtreleme ile paralel web scraping
    """
    query = parsed_query.original_query
    category = parsed_query.category
    budget = parsed_query.budget
    
    logger.info("Paralel web scraping baÅŸlatÄ±lÄ±yor...", query=query)
    
    try:
        # 1. AdÄ±m: Web'de arama yapÄ±p URL'leri topla 
        search_query = f"{query} {category or ''}"
        search_hits = search_products_on_web(search_query, count=30) 
        
        if not search_hits:
            logger.warning("Web aramasÄ±nda sonuÃ§ bulunamadÄ±")
            return []

        # 2. AdÄ±m: Scraping iÃ§in URL listesi hazÄ±rla
        urls_to_scrape = []
        for hit in search_hits:
            url = hit.get("url")
            if url and not any(b in url for b in CONTENT_BLOCKLIST):
                urls_to_scrape.append((url, category, query))
        
        # Maksimum scrape sayÄ±sÄ±nÄ± sÄ±nÄ±rla
        urls_to_scrape = urls_to_scrape[:MAX_SCRAPE_ATTEMPTS]
        
        if not urls_to_scrape:
            logger.warning("Scraping iÃ§in geÃ§erli URL bulunamadÄ±")
            return []
        
        logger.info(f"Paralel scraping baÅŸlÄ±yor: {len(urls_to_scrape)} URL")
        
        # 3. AdÄ±m: Paralel scraping
        scraped_products = []
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # TÃ¼m scraping gÃ¶revlerini baÅŸlat
            future_to_url = {
                executor.submit(_scrape_single_url, url_data): url_data[0] 
                for url_data in urls_to_scrape
            }
            
            # SonuÃ§larÄ± topla (timeout ile)
            completed_count = 0
            for future in as_completed(future_to_url, timeout=SCRAPING_TIMEOUT):
                completed_count += 1
                url = future_to_url[future]
                
                try:
                    result = future.result()
                    if result:
                        scraped_products.append(result)
                        logger.info(f"âœ… [{completed_count}/{len(urls_to_scrape)}] BaÅŸarÄ±lÄ±: {url}")
                    else:
                        logger.debug(f"âŒ [{completed_count}/{len(urls_to_scrape)}] BaÅŸarÄ±sÄ±z: {url}")
                        
                except Exception as e:
                    logger.warning(f"âŒ [{completed_count}/{len(urls_to_scrape)}] Hata {url}: {str(e)}")
        
        logger.info(f"Paralel scraping tamamlandÄ±: {len(scraped_products)} Ã¼rÃ¼n bulundu")
        
        # 4. ADIM: DÃœZELTÄ°LDÄ° - Daha akÄ±llÄ± filtreleme
        filtered_candidates = []
        
        for product in scraped_products:
            product_name = product.get("name", "")
            price = product.get("price")
            
            # Filtre 1: Temel alakasÄ±zlÄ±k kontrolÃ¼ (akÄ±llÄ±)
            if not _is_relevant_product(product_name, category):
                continue
            
            # Filtre 2: BÃ¼tÃ§e kontrolÃ¼ (dinamik tolerans)
            if budget and not _is_price_reasonable(price, budget):
                continue
            
            # Filtre 3: Minimum kalite kontrolÃ¼ (esnek)
            min_price = 3000
            if category and category.lower() == "laptop":
                min_price = 8000  # Laptop iÃ§in biraz daha yÃ¼ksek
            elif category and category.lower() == "telefon":
                min_price = 2000
                
            if not price or price < min_price:
                logger.debug(f"Minimum fiyat kontrolÃ¼ eledi: {price} TL < {min_price} TL")
                continue
            
            logger.info(f"âœ… GeÃ§erli Ã¼rÃ¼n: {product_name[:60]}... - {price} TL")
            filtered_candidates.append(product)
        
        logger.info(f"Filtreleme sonrasÄ± {len(filtered_candidates)} geÃ§erli Ã¼rÃ¼n")
        return filtered_candidates

    except Exception as e:
        logger.error("Paralel web scraping hatasÄ±.", error=str(e), query=query)
        return []

def calculate_product_relevance(product: Dict[str, Any], query: str) -> float:
    """
    GÃœNCELLENDÄ°: PuanÄ± 0-100 arasÄ±na normalize eder.
    """
    if not query or not product:
        return 0.0
    
    score = 0.0
    query_lower = query.lower().strip()
    product_name_lower = (product.get("name") or "").lower()
    specs_text = " ".join(str(v) for v in (product.get("specs") or {}).values()).lower()
    
    # Temel string matching puanÄ±
    if query_lower in product_name_lower:
        score += 100.0
    
    # Kelime bazlÄ± puanlama (aÄŸÄ±rlÄ±klÄ±)
    query_words = set(query_lower.split())
    for word in query_words:
        if len(word) > 2:
            if word in product_name_lower:
                score += 25.0
            if word in specs_text:
                score += 10.0
    
    # Kategori uyumu kontrolÃ¼ (Ã§ok Ã¶nemli)
    parsed_query = parse_query(query)
    if parsed_query.category and parsed_query.category.lower() == "laptop":
        laptop_indicators = ["laptop", "notebook", "gaming", "taÅŸÄ±nabilir", "inc", "inÃ§"]
        desktop_indicators = ["masaÃ¼stÃ¼", "desktop", "hazÄ±r sistem", "gaming pc"]
        
        has_laptop = any(ind in product_name_lower for ind in laptop_indicators)
        has_desktop = any(ind in product_name_lower for ind in desktop_indicators)
        
        if has_desktop and not has_laptop:
            score *= 0.3  # MasaÃ¼stÃ¼ ise puanÄ± dÃ¼ÅŸÃ¼r
        elif has_laptop:
            score += 40.0  # Laptop ise bonus puan
    
    # GPU/Ä°ÅŸlemci puanlamasÄ± (geliÅŸtirildi)
    gpu_terms = ["rtx", "gtx", "radeon", "intel", "amd", "nvidia", "4060", "4070", "3060", "3070"]
    for gpu in gpu_terms:
        if gpu in query_lower and gpu in product_name_lower:
            score += 40.0
    
    # Marka uyumu
    common_brands = ["msi", "asus", "hp", "acer", "lenovo", "dell", "apple", "samsung", "casper"]
    for brand in common_brands:
        if brand in query_lower and brand in product_name_lower:
            score += 20.0
    
    # Fiyat uyumu (iyileÅŸtirildi ve CEZALANDIRMA eklendi)
    price = product.get("price")
    budget_match = re.search(r'(\d+)(?:000|k|bin)', query_lower)
    
    if price and budget_match:
        budget = int(budget_match.group(1)) * 1000
        price_diff = abs(price - budget) / budget
        
        if price > budget * 1.05: # BÃ¼tÃ§enin %5'inden fazlaysa
            # Ne kadar uzaksa o kadar cezalandÄ±r
            penalty_factor = max(0.1, 1 - (price_diff * 1.5)) # Fark arttÄ±kÃ§a ceza artar
            score *= penalty_factor
            logger.debug(f"Fiyat cezasÄ±: {product.get('name')[:30]}... Yeni Puan: {score:.2f} (FaktÃ¶r: {penalty_factor:.2f})")
        elif price <= budget:
            score += 30.0 # BÃ¼tÃ§e altÄ± veya eÅŸitse daha fazla bonus puan
        elif price_diff < 0.10: # BÃ¼tÃ§enin %10 iÃ§indeyse
            score += 15.0

    # Teknik Ã¶zellik zenginliÄŸi
    specs = product.get("specs", {})
    if isinstance(specs, dict) and len(specs) > 5:
        score += 15.0
    
    # YENÄ°: PuanÄ± 0-100 arasÄ±na normalize et
    MAX_SCORE = 250.0
    normalized_score = (min(score, MAX_SCORE) / MAX_SCORE) * 100
    
    return round(normalized_score, 2)

def gather_candidates(query: str, count: int = 10) -> List[Dict[str, Any]]:
    """
    DÃœZELTÄ°LDÄ°: Daha akÄ±llÄ± filtreleme ile paralel scraping
    """
    # Sorguyu en baÅŸta analiz et
    parsed_query = parse_query(query)
    category = parsed_query.category
    
    cache_key = f"{query}-{category}"
    if cache_key in _CACHE:
        cached = _CACHE[cache_key]
        if time.time() - cached["timestamp"] < CACHE_TIMEOUT_SECONDS:
            logger.info("Ã–nbellekten sonuÃ§lar getiriliyor.", query=query)
            return cached["data"][:count]

    start_time = time.time()
    logger.info("Paralel Ã¼rÃ¼n aday arama baÅŸlatÄ±lÄ±yor.", query=query, category=category)

    # 1) WEB: Paralel scraping ve akÄ±llÄ± filtreleme
    web_candidates = _fetch_and_filter_web_candidates_parallel(parsed_query)
    
    # 2) LOCAL: kategori biliniyorsa daralt, deÄŸilse tÃ¼mÃ¼nÃ¼ al
    local_filtered = (
        [p for p in local_products if (p.get("category") or "").lower() == (category or "").lower()]
        if category else list(local_products)
    )
    
    # LOCAL adaylarÄ± da alakasÄ±zlÄ±k kontrolÃ¼nden geÃ§ir (akÄ±llÄ±)
    local_relevant = []
    for p in local_filtered:
        if _is_relevant_product(p.get("name", ""), category):
            local_relevant.append(_ensure_local_source(p))
    
    # 3) BÄ°RLEÅTÄ°R + DEDUPE (web Ã¶ncelikli)
    combined = web_candidates + local_relevant
    seen = set()
    uniq: List[dict] = []
    for p in combined:
        k = _dedupe_key(p)
        if k not in seen:
            uniq.append(p)
            seen.add(k)
    
    # 4) En iyi adaylarÄ± seÃ§ ve sÄ±rala
    sorted_candidates = sorted(
        uniq, 
        key=lambda p: calculate_product_relevance(p, query), 
        reverse=True
    )
    
    # 5) SonuÃ§larÄ± Ã¶nbelleÄŸe kaydet
    _CACHE[cache_key] = {"timestamp": time.time(), "data": sorted_candidates}
    
    # Final log
    elapsed_time = time.time() - start_time
    logger.info(
        f"Paralel aday toplama tamamlandÄ±",
        web_candidates=len(web_candidates),
        local_candidates=len(local_relevant),
        final_count=len(sorted_candidates[:count]),
        elapsed_seconds=f"{elapsed_time:.2f}",
        # Orijinal koddaki bu satÄ±rÄ± koruyoruz
        performance_improvement=f"{((40-elapsed_time)/40)*100:.1f}%" if elapsed_time < 40 else "0%"
    )
    
    return sorted_candidates[:count]


if __name__ == "__main__":
    print("\n--- GELÄ°ÅTÄ°RÄ°LMÄ°Å Paralel Scraping Test: '40000 TL civarÄ± RTX 4060 laptop' ---")
    test_query = "40000 TL civarÄ± RTX 4060 laptop"
    
    start_time = time.time()
    candidates = gather_candidates(test_query, count=8) 
    end_time = time.time()
    
    print(f"\nğŸš€ Toplam sÃ¼re: {end_time - start_time:.2f} saniye")
    print(f"ğŸ“Š Bulunan aday sayÄ±sÄ±: {len(candidates)}")
    
    if not candidates:
        print("Bu kriterlere uygun aday bulunamadÄ±.")
    else:
        print("\nğŸ¯ En Ä°yi Adaylar:")
        for i, c in enumerate(candidates, 1):
            relevance_score = calculate_product_relevance(c, test_query)
            price = c.get('price', 'N/A')
            source = c.get('source', 'Bilinmiyor')
            name = c.get('name', 'Ä°simsiz')[:80]
            print(f"{i}. [{source}] {name}")
            # GÃœNCELLENDÄ°: Etiket ve format gÃ¼ncellendi
            print(f"   ğŸ’° Fiyat: {price} TL | ğŸ¯ Uygunluk Skoru: {relevance_score:.1f}/100")
            if c.get('url'):
                print(f"   ğŸ”— {c['url']}")
            print()

